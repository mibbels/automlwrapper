{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "#### Klassen protecten\n",
    "```\n",
    "import AutoMLWrapper.automlwrapper as aw\n",
    "aw.Configuration.Configuration\n",
    "```\n",
    "\n",
    "#### Erweiterung um Bibliotheken\n",
    "-  @__library.setter\n",
    "    - a) Global/GlobalConfig.yaml\n",
    "        - lib-names\n",
    "        - type mappings\n",
    "    - b) directory durchsuchen \n",
    "- import in AutoMLWrapper anpassen\n",
    "- SEDAR automl.py config locations anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/AutoML/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-23 13:41:58.867326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 13:41:59.443491: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-23 13:41:59.443564: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-23 13:41:59.443570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#from automlwrapper import AutoMLWrapper\n",
    "from AutoMLWrapper.automlwrapper import AutoMLWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/glass.csv')\n",
    "user_hp = {'num_trials': 10, 'time_limit': 111, 'testtest': 123}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = AutoMLWrapper('autosklearn')\n",
    "ask.SetOutputDirectory('test/ask')\n",
    "\n",
    "if 0:\n",
    "    ask.Train(data=df, target_column='Type',\n",
    "        task_type='classification',\n",
    "        data_type='tabular',\n",
    "        problem_type='multiclass',\n",
    "        hyperparameters=user_hp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agl = AutoMLWrapper('autogluon')\n",
    "agl.SetOutputDirectory('test/agl')\n",
    "if 0:\n",
    "    agl.Train(data=df, target_column='Type',\n",
    "       task_type='classification',\n",
    "       data_type='tabular',\n",
    "       problem_type='multiclass',\n",
    "       hyperparameters=user_hp\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.9545454382896423\n",
      "\n",
      "Best val_accuracy So Far: 0.9545454382896423\n",
      "Total elapsed time: 00h 00m 56s\n",
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fa75c630670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fa75c630670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fa75c630670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/54\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fa75c6180d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fa75c6180d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fa75c6180d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.8360 - accuracy: 0.0561\n",
      "Epoch 2/54\n",
      "7/7 [==============================] - 0s 813us/step - loss: 1.7641 - accuracy: 0.1215\n",
      "Epoch 3/54\n",
      "7/7 [==============================] - 0s 962us/step - loss: 1.7130 - accuracy: 0.2336\n",
      "Epoch 4/54\n",
      "7/7 [==============================] - 0s 846us/step - loss: 1.6684 - accuracy: 0.3131\n",
      "Epoch 5/54\n",
      "7/7 [==============================] - 0s 817us/step - loss: 1.6289 - accuracy: 0.3551\n",
      "Epoch 6/54\n",
      "7/7 [==============================] - 0s 895us/step - loss: 1.5934 - accuracy: 0.4299\n",
      "Epoch 7/54\n",
      "7/7 [==============================] - 0s 882us/step - loss: 1.5617 - accuracy: 0.5093\n",
      "Epoch 8/54\n",
      "7/7 [==============================] - 0s 800us/step - loss: 1.5324 - accuracy: 0.5607\n",
      "Epoch 9/54\n",
      "7/7 [==============================] - 0s 871us/step - loss: 1.5047 - accuracy: 0.5981\n",
      "Epoch 10/54\n",
      "7/7 [==============================] - 0s 874us/step - loss: 1.4781 - accuracy: 0.5981\n",
      "Epoch 11/54\n",
      "7/7 [==============================] - 0s 834us/step - loss: 1.4523 - accuracy: 0.6121\n",
      "Epoch 12/54\n",
      "7/7 [==============================] - 0s 898us/step - loss: 1.4269 - accuracy: 0.6308\n",
      "Epoch 13/54\n",
      "7/7 [==============================] - 0s 856us/step - loss: 1.4015 - accuracy: 0.6355\n",
      "Epoch 14/54\n",
      "7/7 [==============================] - 0s 793us/step - loss: 1.3758 - accuracy: 0.6262\n",
      "Epoch 15/54\n",
      "7/7 [==============================] - 0s 825us/step - loss: 1.3496 - accuracy: 0.6215\n",
      "Epoch 16/54\n",
      "7/7 [==============================] - 0s 782us/step - loss: 1.3230 - accuracy: 0.6215\n",
      "Epoch 17/54\n",
      "7/7 [==============================] - 0s 755us/step - loss: 1.2960 - accuracy: 0.6262\n",
      "Epoch 18/54\n",
      "7/7 [==============================] - 0s 733us/step - loss: 1.2684 - accuracy: 0.6355\n",
      "Epoch 19/54\n",
      "7/7 [==============================] - 0s 842us/step - loss: 1.2404 - accuracy: 0.6495\n",
      "Epoch 20/54\n",
      "7/7 [==============================] - 0s 756us/step - loss: 1.2123 - accuracy: 0.6495\n",
      "Epoch 21/54\n",
      "7/7 [==============================] - 0s 745us/step - loss: 1.1844 - accuracy: 0.6682\n",
      "Epoch 22/54\n",
      "7/7 [==============================] - 0s 799us/step - loss: 1.1570 - accuracy: 0.6869\n",
      "Epoch 23/54\n",
      "7/7 [==============================] - 0s 769us/step - loss: 1.1307 - accuracy: 0.6916\n",
      "Epoch 24/54\n",
      "7/7 [==============================] - 0s 720us/step - loss: 1.1054 - accuracy: 0.6822\n",
      "Epoch 25/54\n",
      "7/7 [==============================] - 0s 737us/step - loss: 1.0815 - accuracy: 0.6916\n",
      "Epoch 26/54\n",
      "7/7 [==============================] - 0s 764us/step - loss: 1.0589 - accuracy: 0.6963\n",
      "Epoch 27/54\n",
      "7/7 [==============================] - 0s 872us/step - loss: 1.0377 - accuracy: 0.6916\n",
      "Epoch 28/54\n",
      "7/7 [==============================] - 0s 758us/step - loss: 1.0176 - accuracy: 0.6963\n",
      "Epoch 29/54\n",
      "7/7 [==============================] - 0s 775us/step - loss: 0.9985 - accuracy: 0.6963\n",
      "Epoch 30/54\n",
      "7/7 [==============================] - 0s 789us/step - loss: 0.9806 - accuracy: 0.6963\n",
      "Epoch 31/54\n",
      "7/7 [==============================] - 0s 778us/step - loss: 0.9637 - accuracy: 0.6963\n",
      "Epoch 32/54\n",
      "7/7 [==============================] - 0s 768us/step - loss: 0.9477 - accuracy: 0.6916\n",
      "Epoch 33/54\n",
      "7/7 [==============================] - 0s 750us/step - loss: 0.9325 - accuracy: 0.6916\n",
      "Epoch 34/54\n",
      "7/7 [==============================] - 0s 773us/step - loss: 0.9182 - accuracy: 0.6916\n",
      "Epoch 35/54\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.9048 - accuracy: 0.6963\n",
      "Epoch 36/54\n",
      "7/7 [==============================] - 0s 971us/step - loss: 0.8919 - accuracy: 0.7009\n",
      "Epoch 37/54\n",
      "7/7 [==============================] - 0s 974us/step - loss: 0.8796 - accuracy: 0.7009\n",
      "Epoch 38/54\n",
      "7/7 [==============================] - 0s 786us/step - loss: 0.8680 - accuracy: 0.7056\n",
      "Epoch 39/54\n",
      "7/7 [==============================] - 0s 822us/step - loss: 0.8567 - accuracy: 0.7103\n",
      "Epoch 40/54\n",
      "7/7 [==============================] - 0s 788us/step - loss: 0.8460 - accuracy: 0.7150\n",
      "Epoch 41/54\n",
      "7/7 [==============================] - 0s 854us/step - loss: 0.8357 - accuracy: 0.7150\n",
      "Epoch 42/54\n",
      "7/7 [==============================] - 0s 766us/step - loss: 0.8259 - accuracy: 0.7196\n",
      "Epoch 43/54\n",
      "7/7 [==============================] - 0s 754us/step - loss: 0.8164 - accuracy: 0.7196\n",
      "Epoch 44/54\n",
      "7/7 [==============================] - 0s 815us/step - loss: 0.8071 - accuracy: 0.7243\n",
      "Epoch 45/54\n",
      "7/7 [==============================] - 0s 798us/step - loss: 0.7982 - accuracy: 0.7290\n",
      "Epoch 46/54\n",
      "7/7 [==============================] - 0s 751us/step - loss: 0.7895 - accuracy: 0.7290\n",
      "Epoch 47/54\n",
      "7/7 [==============================] - 0s 766us/step - loss: 0.7811 - accuracy: 0.7336\n",
      "Epoch 48/54\n",
      "7/7 [==============================] - 0s 811us/step - loss: 0.7729 - accuracy: 0.7336\n",
      "Epoch 49/54\n",
      "7/7 [==============================] - 0s 806us/step - loss: 0.7650 - accuracy: 0.7336\n",
      "Epoch 50/54\n",
      "7/7 [==============================] - 0s 747us/step - loss: 0.7573 - accuracy: 0.7430\n",
      "Epoch 51/54\n",
      "7/7 [==============================] - 0s 741us/step - loss: 0.7498 - accuracy: 0.7477\n",
      "Epoch 52/54\n",
      "7/7 [==============================] - 0s 885us/step - loss: 0.7425 - accuracy: 0.7523\n",
      "Epoch 53/54\n",
      "7/7 [==============================] - 0s 778us/step - loss: 0.7354 - accuracy: 0.7570\n",
      "Epoch 54/54\n",
      "7/7 [==============================] - 0s 766us/step - loss: 0.7285 - accuracy: 0.7617\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fa75c577ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fa75c577ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fa75c577ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fa7446addc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fa7446addc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fa7446addc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test/akr/structured_data_classifier/best_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test/akr/structured_data_classifier/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "akr = AutoMLWrapper('autokeras')\n",
    "akr.SetOutputDirectory('test/akr')\n",
    "if 1:\n",
    "    akr.Train(data=df, target_column='Type',\n",
    "        task_type='classification',\n",
    "        data_type='tabular',\n",
    "        problem_type='multiclass',\n",
    "        hyperparameters=user_hp\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
